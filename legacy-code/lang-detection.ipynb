{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.9.0 fastapi uvicorn langid fasttext langdetect requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Union, List, Dict\n",
    "\n",
    "class LanguageSearchOption(BaseModel):\n",
    "    texts: Union[str, List[str]] = Field(example=\"text\")\n",
    "    model_name: str = Field(default=\"fasttext\")\n",
    "    max_value: int = Field(default=3)\n",
    "\n",
    "class LanguageResponseModel(BaseModel):\n",
    "    body: List[Dict[str, List[Dict[str, float]]]]\n",
    "    status: int = Field(example=200, description=\"Status of the request\")\n",
    "    message: str = Field(\n",
    "        example=\"message\", description=\"Attached message for the request\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "def http_get(url, path):\n",
    "    if os.path.dirname(path) != '':\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    req = requests.get(url, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Exception when trying to download {}. Response {}\".format(url, req.status_code), file=sys.stderr)\n",
    "        req.raise_for_status()\n",
    "        return\n",
    "\n",
    "    download_filepath = path+\"_part\"\n",
    "    with open(download_filepath, \"wb\") as file_binary:\n",
    "        content_length = req.headers.get('Content-Length')\n",
    "        total = int(content_length) if content_length is not None else None\n",
    "        progress = tqdm.tqdm(unit=\"B\", total=total, unit_scale=True)\n",
    "        for chunk in req.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                progress.update(len(chunk))\n",
    "                file_binary.write(chunk)\n",
    "\n",
    "    os.rename(download_filepath, path)\n",
    "    progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Tuple\n",
    "import torch\n",
    "\n",
    "class LanguageDetector:\n",
    "    def __init__(self, model_name: str = \"FastText\", cache_folder: str = None):\n",
    "        self._fasttext_lang_id = None\n",
    "        if model_name.lower() == \"fasttext\":\n",
    "          self._lang_detectors = self.language_detection_fasttext\n",
    "        elif model_name == \"langdetect\":\n",
    "          self._lang_detectors = self.language_detection_langdetect\n",
    "        elif model_name == \"langid\":\n",
    "          self._lang_detectors = self.language_detection_langid\n",
    "        else:\n",
    "          raise ValueError(\"unknown option\")\n",
    "\n",
    "        if cache_folder is None:\n",
    "            if 'LD_CACHE' in os.environ:\n",
    "                cache_folder = os.environ['LD_CACHE']\n",
    "            else:\n",
    "                cache_folder = os.path.join(torch.hub._get_torch_home(), 'ld_v2')\n",
    "        self._cache_folder = cache_folder\n",
    "\n",
    "    def language_detection_fasttext(self, text: str, number:int= 3) -> Tuple[str, float]:\n",
    "        if self._fasttext_lang_id is None:\n",
    "          import fasttext\n",
    "          fasttext.FastText.eprint = lambda x: None\n",
    "          model_path = os.path.join(self._cache_folder, 'lid.176.ftz')\n",
    "          self._fasttext_lang_id = fasttext.load_model(model_path)\n",
    "\n",
    "        prediction = self._fasttext_lang_id.predict(text.lower().replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").strip(), k=number)\n",
    "        res = []\n",
    "        for i in range(number):\n",
    "          res.append({prediction[0][i].split('__')[-1] : prediction[1][i]})\n",
    "        return res\n",
    "\n",
    "    def language_detection_langdetect(self, text: str, number:int=3) -> str:\n",
    "        import langdetect\n",
    "        res = []\n",
    "        for lang in langdetect.detect_langs(text.lower()):\n",
    "          info_pred = str(lang).split(\":\")\n",
    "          if len(res) < number:\n",
    "            res.append({info_pred[0] : info_pred[1]})\n",
    "          else:\n",
    "            break\n",
    "        return res\n",
    "\n",
    "    def language_detection_langid(self, text: str, number:int= 3) -> str:\n",
    "        import langid\n",
    "        if not hasattr(self, \"_langid_identifier\"):\n",
    "          self._langid_identifier = langid.langid.LanguageIdentifier.from_modelstring(langid.langid.model, norm_probs=True)\n",
    "        identifier = self._langid_identifier\n",
    "        res = identifier.rank(text.lower().replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").strip())\n",
    "        predictions = [{pred[0]: pred[1]} for pred in res[:number]]\n",
    "        return predictions\n",
    "\n",
    "    def language_detection(self, texts: Union[str, List[str]], max_value: int) -> str:\n",
    "        if isinstance(texts, list):\n",
    "            return [{doc : self.language_detection(texts=doc,max_value=max_value)} for doc in texts]\n",
    "        try:\n",
    "          return self._lang_detectors(text=texts, number=max_value)\n",
    "        except:\n",
    "          raise Exception(\"This method might not be installed, please check with the dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'LD_CACHE' in os.environ:\n",
    "    CACHE_FOLDER = os.environ['LD_CACHE']\n",
    "else:\n",
    "    CACHE_FOLDER = os.path.join(torch.hub._get_torch_home(), 'ld_v2')\n",
    "model_path = os.path.join(CACHE_FOLDER, 'lid.176.ftz')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    http_get('https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "options: LanguageSearchOption = LanguageSearchOption(\n",
    "    texts=\"This is a test\",\n",
    "    model_name=\"fasttext\",\n",
    "    max_value=3\n",
    ")\n",
    "\n",
    "language_detector = LanguageDetector(model_name=options.model_name)\n",
    "res = language_detector.language_detection(texts=options.texts, max_value=options.max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'en': 0.9818459153175354}, {'bn': 0.002121715107932687}, {'hi': 0.0012636820320039988}]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
